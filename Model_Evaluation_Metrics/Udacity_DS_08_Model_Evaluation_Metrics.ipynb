{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583333333333334"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import statements \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the train test split\n",
    "# http://scikit-learn.org/0.16/modules/generated/sklearn.cross_validation.train_test_split.html\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data.\n",
    "data = np.asarray(pd.read_csv('mpm-data.csv', header=None))\n",
    "# Assign the features to the variable X, and the labels to the variable y. \n",
    "X = data[:,0:2]\n",
    "y = data[:,2]\n",
    "\n",
    "# Use train test split to split your data \n",
    "# Use a test size of 25% and a random state of 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# TODO: Create the decision tree model and assign it to the variable model.\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# TODO: Fit the model to the training data.\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# TODO: Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# TODO: Calculate the accuracy and assign it to the variable acc. on the test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6668797696184305"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = 0.556\n",
    "recall = 0.833\n",
    "f1_score = (2 * precision * recall) / (precision + recall)\n",
    "f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6945"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average = (precision + recall) / 2\n",
    "average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f1_score < average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Classification Metrics\n",
    "see [Udacity_DS_08_Classification_Metrics](Udacity_DS_08_Model_Evaluation_Metrics/Udacity_DS_08_Classification_Metrics.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Regression Metrics\n",
    "see [Udacity_DS_08_Regression_Metrics](Udacity_DS_08_Model_Evaluation_Metrics/Udacity_DS_08_Regression_Metrics.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
